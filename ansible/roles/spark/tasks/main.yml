
- name: check spark downloaded
  command: "test -f {{ cache.local }}/{{ package }}"
  delegate_to: localhost
  register: spark_downloaded
  failed_when: spark_downloaded.rc not in [0, 1]
  changed_when: false
  run_once: true

- name: download spark 
  command: get_url url="{{ spark.mirror }}/{{ package }}" dest="{{ cache.local }}"
  delegate_to: localhost
  when: spark_downloaded.rc == 1
  run_once: true

- name: create group
  group:
    name: spark
    state: present

- name: create user
  user:
    name: spark
    group: spark

- name: create spark working directory
  file:
    path: "{{ spark.datadir }}"
    state: directory
    owner: spark
    group: spark

- name: unarchive to the install directory
  unarchive:
    src: "{{ cache.local }}/{{ package }}"
    dest: /opt
    owner: spark
    group: spark
    creates: "{{ spark_path }}/RELEASE"

- name: create link
  file:
    src: "{{ spark_path }}"
    dest: /opt/spark
    state: link

- name: set spark-env.sh
  template:
    src: "spark-env-sh.j2"
    dest: "{{ spark_path }}/conf/spark-env.sh"

- name: set spark-defaults.conf
  template:
    src: "spark-defaults-conf.j2"
    dest: "{{ spark_path }}/conf/spark-defaults.conf"

- name: copy master systemd start scripts
  template:
    src: "spark-master-systemd.j2"
    dest: /etc/systemd/system/spark-master.service
    owner: root
    group: root
  when: inventory_hostname in groups['spark-master'] 
  notify: 
    - restart spark-master

- name: copy worker systemd start scripts
  template:
    src: "spark-worker-systemd.j2"
    dest: /etc/systemd/system/spark-worker.service
    owner: root
    group: root
  when: inventory_hostname in groups['spark-worker'] 
  notify: 
    - restart spark-worker

- name: start spark masters
  service: name=spark-master state=started enabled=yes 
  when: inventory_hostname in groups['spark-master']

- name: start spark workers
  service: name=spark-worker state=started enabled=yes 
  when: inventory_hostname in groups['spark-worker']
